{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNJYkdugOcHdgyIs+zz46vE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"JtTKuP3HozMj","executionInfo":{"status":"ok","timestamp":1698939362232,"user_tz":-480,"elapsed":1030,"user":{"displayName":"james au","userId":"00783333098061234187"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","\n","from torchvision.io import read_image\n","from torchvision.ops.boxes import masks_to_boxes\n","from torchvision import tv_tensors\n","from torchvision.transforms.v2 import functional as F\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"Df4BNMbJIQcw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698939365528,"user_tz":-480,"elapsed":2418,"user":{"displayName":"james au","userId":"00783333098061234187"}},"outputId":"f362fd5b-5f2b-4f36-d4ed-2cded762ff2d"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# !ls drive/MyDrive/'Computational Imaging Project - DTU 34269 - Low Light Super Resolution'\n","!ls drive/MyDrive/'Colab Notebooks'"],"metadata":{"id":"0KBgue1tMtlz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698939365529,"user_tz":-480,"elapsed":7,"user":{"displayName":"james au","userId":"00783333098061234187"}},"outputId":"02ea84ab-d45d-4fbe-96dc-9508b481f7ef"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access 'drive/MyDrive/Colab Notebooks': No such file or directory\n"]}]},{"cell_type":"code","source":["# !unzip gdrive/MyDrive/'Computational Imaging Project - DTU 34269 - Low Light Super Resolution'/Data.zip\n","!unzip gdrive/MyDrive/'Colab Notebooks'/PennFudanPed.zip"],"metadata":{"id":"1eeCsTkZMudM","executionInfo":{"status":"ok","timestamp":1698939711869,"user_tz":-480,"elapsed":346343,"user":{"displayName":"james au","userId":"00783333098061234187"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eb23d935-664c-424a-e810-07cbfd080303"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  gdrive/MyDrive/Colab Notebooks/PennFudanPed.zip\n","replace PennFudanPed/added-object-list.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"code","source":["# ImageFile_Path = 'data/PennFudanPed'\n","ImageFile_Path = 'PennFudanPed'"],"metadata":{"id":"I_fpAldmLikN","executionInfo":{"status":"ok","timestamp":1698939939740,"user_tz":-480,"elapsed":2,"user":{"displayName":"james au","userId":"00783333098061234187"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","execution_count":67,"metadata":{"id":"vMKuQ4YayII5","executionInfo":{"status":"ok","timestamp":1698939943991,"user_tz":-480,"elapsed":856,"user":{"displayName":"james au","userId":"00783333098061234187"}}},"outputs":[],"source":["\n","\n","\n","class PennFudanDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms):\n","        self.root = root\n","        self.transforms = transforms\n","        # load all image files, sorting them to\n","        # ensure that they are aligned\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n","\n","    def __getitem__(self, idx):\n","        # load images and masks\n","        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n","        img = read_image(img_path)\n","        mask = read_image(mask_path)\n","        # instances are encoded as different colors\n","        obj_ids = torch.unique(mask)\n","        # first id is the background, so remove it\n","        obj_ids = obj_ids[1:]\n","        num_objs = len(obj_ids)\n","\n","        # split the color-encoded mask into a set\n","        # of binary masks\n","        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n","\n","        # get bounding box coordinates for each mask\n","        boxes = masks_to_boxes(masks)\n","\n","        # there is only one class\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","\n","        image_id = idx\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        # Wrap sample and targets into torchvision tv_tensors:\n","        img = tv_tensors.Image(img)\n","\n","        target = {}\n","        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n","        target[\"masks\"] = tv_tensors.Mask(masks)\n","        target[\"labels\"] = labels\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)"]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","# load a model pre-trained on COCO\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","# replace the classifier with a new one, that has\n","# num_classes which is user-defined\n","num_classes = 2  # 1 class (person) + background\n","# get number of input features for the classifier\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# replace the pre-trained head with a new one\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"],"metadata":{"id":"N2MbmMjhyaab","executionInfo":{"status":"ok","timestamp":1698939950957,"user_tz":-480,"elapsed":2528,"user":{"displayName":"james au","userId":"00783333098061234187"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# load a pre-trained model for classification and return\n","# only the features\n","backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n","# ``FasterRCNN`` needs to know the number of\n","# output channels in a backbone. For mobilenet_v2, it's 1280\n","# so we need to add it here\n","backbone.out_channels = 1280\n","\n","# let's make the RPN generate 5 x 3 anchors per spatial\n","# location, with 5 different sizes and 3 different aspect\n","# ratios. We have a Tuple[Tuple[int]] because each feature\n","# map could potentially have different sizes and\n","# aspect ratios\n","anchor_generator = AnchorGenerator(\n","    sizes=((32, 64, 128, 256, 512),),\n","    aspect_ratios=((0.5, 1.0, 2.0),)\n",")\n","\n","# let's define what are the feature maps that we will\n","# use to perform the region of interest cropping, as well as\n","# the size of the crop after rescaling.\n","# if your backbone returns a Tensor, featmap_names is expected to\n","# be [0]. More generally, the backbone should return an\n","# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n","# feature maps to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","    featmap_names=['0'],\n","    output_size=7,\n","    sampling_ratio=2,\n",")\n","\n","# put the pieces together inside a Faster-RCNN model\n","model = FasterRCNN(\n","    backbone,\n","    num_classes=2,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler,\n",")"],"metadata":{"id":"J1TmMPZVyjQ3","executionInfo":{"status":"ok","timestamp":1698939953298,"user_tz":-480,"elapsed":2358,"user":{"displayName":"james au","userId":"00783333098061234187"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained on COCO\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # now get the number of input features for the mask classifier\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # and replace the mask predictor with a new one\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n","        in_features_mask,\n","        hidden_layer,\n","        num_classes,\n","    )\n","\n","    return model"],"metadata":{"id":"oJCNKjsH2c9d","executionInfo":{"status":"ok","timestamp":1698939953298,"user_tz":-480,"elapsed":3,"user":{"displayName":"james au","userId":"00783333098061234187"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["from torchvision.transforms import v2 as T\n","\n","\n","def get_transform(train):\n","    transforms = []\n","    if train:\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    transforms.append(T.ToDtype(torch.float, scale=True))\n","    transforms.append(T.ToPureTensor())\n","    return T.Compose(transforms)"],"metadata":{"id":"nFuyQyYB2jId","executionInfo":{"status":"ok","timestamp":1698939958158,"user_tz":-480,"elapsed":3,"user":{"displayName":"james au","userId":"00783333098061234187"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n","os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n","os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n","os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n","os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NCTGYj6Oq41t","executionInfo":{"status":"ok","timestamp":1698939961779,"user_tz":-480,"elapsed":704,"user":{"displayName":"james au","userId":"00783333098061234187"}},"outputId":"d6761f5b-4532-49df-df4d-1b6c9c819467"},"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["# !pip install utils"],"metadata":{"id":"B68XF0gPPCjS","executionInfo":{"status":"aborted","timestamp":1698939933556,"user_tz":-480,"elapsed":5,"user":{"displayName":"james au","userId":"00783333098061234187"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download TorchVision repo to use some files from\n","# references/detection\n","# !git clone\n","# !cd vision\n","# !git checkout v0.16\n","# !cp references/detection"],"metadata":{"id":"oPaMU7b3WTmh","executionInfo":{"status":"aborted","timestamp":1698939933556,"user_tz":-480,"elapsed":5,"user":{"displayName":"james au","userId":"00783333098061234187"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install git+https://github.com/pytorch/vision.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QRQHyLamkq5X","executionInfo":{"status":"ok","timestamp":1698939879085,"user_tz":-480,"elapsed":163854,"user":{"displayName":"james au","userId":"00783333098061234187"}},"outputId":"ffabbb9c-f62e-4673-f8c5-2ee1644d7c39"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/pytorch/vision.git\n","  Cloning https://github.com/pytorch/vision.git to /tmp/pip-req-build-yl3al2i4\n","  Running command git clone --filter=blob:none --quiet https://github.com/pytorch/vision.git /tmp/pip-req-build-yl3al2i4\n","  Resolved https://github.com/pytorch/vision.git to commit f69eee6108cd047ac8b62a2992244e9ab3c105e1\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0a0+f69eee6) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0a0+f69eee6) (2.31.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0a0+f69eee6) (2.1.0+cu118)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0a0+f69eee6) (9.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0a0+f69eee6) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0a0+f69eee6) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0a0+f69eee6) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0a0+f69eee6) (2023.7.22)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchvision==0.17.0a0+f69eee6) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchvision==0.17.0a0+f69eee6) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchvision==0.17.0a0+f69eee6) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchvision==0.17.0a0+f69eee6) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchvision==0.17.0a0+f69eee6) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchvision==0.17.0a0+f69eee6) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchvision==0.17.0a0+f69eee6) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchvision==0.17.0a0+f69eee6) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchvision==0.17.0a0+f69eee6) (1.3.0)\n","Building wheels for collected packages: torchvision\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for torchvision \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for torchvision (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for torchvision\u001b[0m\u001b[31m\n","\u001b[0mFailed to build torchvision\n","\u001b[31mERROR: Could not build wheels for torchvision, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# import utils\n","# from detection import utils\n","# import torchvision.models.detection.\n","# import torch.\n","# import\n","# from content.vision.references.detection import utils #as utils\n","\n","from vision.references.detection import utils #as utils\n","# from /content/vision/references"],"metadata":{"id":"31sU25NRXMSr","executionInfo":{"status":"ok","timestamp":1698940177515,"user_tz":-480,"elapsed":5,"user":{"displayName":"james au","userId":"00783333098061234187"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["\n","\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","dataset = PennFudanDataset(ImageFile_Path, get_transform(train=True))\n","data_loader = torch.utils.data.DataLoader(\n","    dataset,\n","    batch_size=2,\n","    shuffle=True,\n","    num_workers=4,\n","    collate_fn=utils.collate_fn\n",")\n","\n","# For Training\n","images, targets = next(iter(data_loader))\n","images = list(image for image in images)\n","targets = [{k: v for k, v in t.items()} for t in targets]\n","output = model(images, targets)  # Returns losses and detections\n","print(output)\n","\n","# For inference\n","model.eval()\n","x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","predictions = model(x)  # Returns predictions\n","print(predictions[0])"],"metadata":{"id":"GuBFW2E32lgf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698940217485,"user_tz":-480,"elapsed":35267,"user":{"displayName":"james au","userId":"00783333098061234187"}},"outputId":"600b3664-1570-4752-f188-4a2b2f1b899e"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["{'loss_classifier': tensor(0.1778, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0211, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0216, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0013, grad_fn=<DivBackward0>)}\n","{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)}\n"]}]},{"cell_type":"code","source":["# !pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\n","# from coco_eval import CocoEvaluator\n","# from vision.references.detection import *\n","# from vision.references.detection.engine import train_one_epoch, evaluate\n","from engine import train_one_epoch, evaluate\n","\n","# train on the GPU or on the CPU, if a GPU is not available\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# our dataset has two classes only - background and person\n","num_classes = 2\n","# use our dataset and defined transformations\n","dataset = PennFudanDataset(ImageFile_Path, get_transform(train=True))\n","dataset_test = PennFudanDataset(ImageFile_Path, get_transform(train=False))\n","\n","# split the dataset in train and test set\n","indices = torch.randperm(len(dataset)).tolist()\n","dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset,\n","    batch_size=2,\n","    shuffle=True,\n","    num_workers=4,\n","    collate_fn=utils.collate_fn\n",")\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test,\n","    batch_size=1,\n","    shuffle=False,\n","    num_workers=4,\n","    collate_fn=utils.collate_fn\n",")\n","\n","# get the model using our helper function\n","model = get_model_instance_segmentation(num_classes)\n","\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(\n","    params,\n","    lr=0.005,\n","    momentum=0.9,\n","    weight_decay=0.0005\n",")\n","\n","# and a learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(\n","    optimizer,\n","    step_size=3,\n","    gamma=0.1\n",")\n","\n","# let's train it for 5 epochs\n","num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    evaluate(model, data_loader_test, device=device)\n","\n","print(\"That's it!\")"],"metadata":{"id":"r_NdXK8S2q0w","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"error","timestamp":1698940248346,"user_tz":-480,"elapsed":1682,"user":{"displayName":"james au","userId":"00783333098061234187"}},"outputId":"b3754a8a-5520-4287-b001-56cd813e42e1"},"execution_count":78,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-40b03db08db6>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmetric_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetricLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_meter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSmoothedValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{value:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Epoch: [{epoch}]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'utils' has no attribute 'MetricLogger'"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n","\n","image = read_image(\"../_static/img/tv_tutorial/tv_image05.png\")\n","eval_transform = get_transform(train=False)\n","\n","model.eval()\n","with torch.no_grad():\n","    x = eval_transform(image)\n","    # convert RGBA -> RGB and move to device\n","    x = x[:3, ...].to(device)\n","    predictions = model([x, ])\n","    pred = predictions[0]\n","\n","image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n","image = image[:3, ...]\n","pred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n","pred_boxes = pred[\"boxes\"].long()\n","output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n","\n","masks = (pred[\"masks\"] > 0.7).squeeze(1)\n","output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n","\n","plt.figure(figsize=(12, 12))\n","plt.imshow(output_image.permute(1, 2, 0))"],"metadata":{"id":"RQllpMej2uqF","executionInfo":{"status":"aborted","timestamp":1698939933554,"user_tz":-480,"elapsed":7,"user":{"displayName":"james au","userId":"00783333098061234187"}}},"execution_count":null,"outputs":[]}]}